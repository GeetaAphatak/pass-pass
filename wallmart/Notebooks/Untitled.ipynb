{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3594f939-49cc-448e-898f-cbdf8aa9e0f0",
   "metadata": {},
   "source": [
    "Plan for developing the anomaly detection model for your weekly sales data:\n",
    "\n",
    "### Plan Outline:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Load the data and convert the 'Date' column to a datetime format.\n",
    "   - Sort the data by state and date to ensure the time series is in the correct order.\n",
    "   - Handle missing values if necessary (e.g., fill or remove).\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Group data by state as operations need to be state-wise.\n",
    "   - Create lag features for 'Weekly_Sales' to capture previous sales trends (consider several weeks back).\n",
    "   - Consider the impact of 'Holiday_Flag' in feature creation to capture holiday sales effects.\n",
    "\n",
    "3. **Model Selection**:\n",
    "   - Given the need for high precision and good recall, consider models well-suited for anomaly detection in time series data. Potential candidates include:\n",
    "     - **Isolation Forest**\n",
    "     - **Random Forest Classifier**\n",
    "     - **Gradient Boosting Machines (e.g., XGBoost)**\n",
    "\n",
    "4. **Training and Validation**:\n",
    "   - Split the data into training (60%), validation (20%), and test (20%) sets. Ensure this split respects the time series nature, meaning no future data is used in the training or validation of earlier data.\n",
    "   - Train the models on the training set.\n",
    "   - Evaluate each model on the validation set using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. **Final Model Selection**:\n",
    "   - Select the model that best achieves the balance between precision and recall as specified.\n",
    "   - Retrain this model on combined training and validation sets.\n",
    "   - Perform a final evaluation on the test set.\n",
    "\n",
    "6. **Model Deployment**:\n",
    "   - Prepare the model and the preprocessing steps to be applied to new or unseen future data for predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87e2142-d882-46c3-9e36-15b35df1e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc55fec2-004c-40fd-9ccf-53607b0f2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'C:\\\\Geeta\\\\learning\\\\projects\\\\AnomalyDetectionSXM\\\\Notebooks\\\\Datasets\\\\Pipeline'\n",
    "dataset_name = 'Walmart_Weekly'\n",
    "train_file = base_folder + '/train/' + dataset_name +'_train.csv'\n",
    "inference_file = base_folder + '/inference/' + dataset_name +'_inference.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d356c283-d3fb-4b91-8f59-7d702281cdeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>State</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2021-02-05</td>\n",
       "      <td>12535669.50</td>\n",
       "      <td>32.53</td>\n",
       "      <td>2.77</td>\n",
       "      <td>150.50</td>\n",
       "      <td>8.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13672907.31</td>\n",
       "      <td>11591637.96</td>\n",
       "      <td>California</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2021-02-12</td>\n",
       "      <td>11788052.22</td>\n",
       "      <td>32.71</td>\n",
       "      <td>2.75</td>\n",
       "      <td>150.58</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12967716.55</td>\n",
       "      <td>10886447.20</td>\n",
       "      <td>California</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>11969069.14</td>\n",
       "      <td>37.00</td>\n",
       "      <td>2.73</td>\n",
       "      <td>150.62</td>\n",
       "      <td>8.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12691307.91</td>\n",
       "      <td>10610038.55</td>\n",
       "      <td>California</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>11131071.10</td>\n",
       "      <td>35.52</td>\n",
       "      <td>2.73</td>\n",
       "      <td>150.66</td>\n",
       "      <td>8.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12804766.72</td>\n",
       "      <td>10723497.37</td>\n",
       "      <td>California</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>11680802.02</td>\n",
       "      <td>41.10</td>\n",
       "      <td>2.77</td>\n",
       "      <td>150.69</td>\n",
       "      <td>8.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12610512.80</td>\n",
       "      <td>10529243.45</td>\n",
       "      <td>California</td>\n",
       "      <td>3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>2023-09-22</td>\n",
       "      <td>6851231.75</td>\n",
       "      <td>70.84</td>\n",
       "      <td>3.90</td>\n",
       "      <td>175.44</td>\n",
       "      <td>7.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7771717.32</td>\n",
       "      <td>6550932.76</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>9</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>6801804.41</td>\n",
       "      <td>70.62</td>\n",
       "      <td>3.84</td>\n",
       "      <td>175.57</td>\n",
       "      <td>7.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7538640.58</td>\n",
       "      <td>6317856.02</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>9</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>2023-10-06</td>\n",
       "      <td>7354905.93</td>\n",
       "      <td>68.47</td>\n",
       "      <td>3.84</td>\n",
       "      <td>175.69</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7571680.67</td>\n",
       "      <td>6350896.11</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>2023-10-13</td>\n",
       "      <td>7149626.82</td>\n",
       "      <td>61.82</td>\n",
       "      <td>3.92</td>\n",
       "      <td>175.82</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7860594.00</td>\n",
       "      <td>6639809.44</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>2023-10-20</td>\n",
       "      <td>7009848.14</td>\n",
       "      <td>62.51</td>\n",
       "      <td>3.91</td>\n",
       "      <td>175.85</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7584779.90</td>\n",
       "      <td>6363995.33</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "142 2021-02-05   12535669.50        32.53        2.77  150.50          8.84   \n",
       "143 2021-02-12   11788052.22        32.71        2.75  150.58          8.84   \n",
       "144 2021-02-19   11969069.14        37.00        2.73  150.62          8.84   \n",
       "145 2021-02-26   11131071.10        35.52        2.73  150.66          8.84   \n",
       "146 2021-03-05   11680802.02        41.10        2.77  150.69          8.84   \n",
       "..         ...           ...          ...         ...     ...           ...   \n",
       "705 2023-09-22    6851231.75        70.84        3.90  175.44          7.26   \n",
       "706 2023-09-29    6801804.41        70.62        3.84  175.57          7.26   \n",
       "707 2023-10-06    7354905.93        68.47        3.84  175.69          6.96   \n",
       "708 2023-10-13    7149626.82        61.82        3.92  175.82          6.96   \n",
       "709 2023-10-20    7009848.14        62.51        3.91  175.85          6.96   \n",
       "\n",
       "     Holiday_Flag  Anomaly  Sales_Amount_Upper  Sales_Amount_Lower  \\\n",
       "142             0        0         13672907.31         11591637.96   \n",
       "143             1        0         12967716.55         10886447.20   \n",
       "144             0        0         12691307.91         10610038.55   \n",
       "145             0        0         12804766.72         10723497.37   \n",
       "146             0        0         12610512.80         10529243.45   \n",
       "..            ...      ...                 ...                 ...   \n",
       "705             0        0          7771717.32          6550932.76   \n",
       "706             0        0          7538640.58          6317856.02   \n",
       "707             0        0          7571680.67          6350896.11   \n",
       "708             0        0          7860594.00          6639809.44   \n",
       "709             0        0          7584779.90          6363995.33   \n",
       "\n",
       "          State  month  year  \n",
       "142  California      2  2021  \n",
       "143  California      2  2021  \n",
       "144  California      2  2021  \n",
       "145  California      2  2021  \n",
       "146  California      3  2021  \n",
       "..          ...    ...   ...  \n",
       "705    Virginia      9  2023  \n",
       "706    Virginia      9  2023  \n",
       "707    Virginia     10  2023  \n",
       "708    Virginia     10  2023  \n",
       "709    Virginia     10  2023  \n",
       "\n",
       "[710 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = train_file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='mixed') #format='%Y-%m-%d')\n",
    "\n",
    "# Sort data by state and date\n",
    "data.sort_values(by=['State', 'Date'], inplace=True)\n",
    "\n",
    "# Handling missing values (simple forward fill as a placeholder)\n",
    "data.ffill(inplace=True)\n",
    "\n",
    "data['month'] = data['Date'].dt.month\n",
    "data['year'] = data['Date'].dt.year\n",
    "\n",
    "# Remove rows with NaN values created by lag features\n",
    "data.dropna(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7d7161-7257-4a3c-b74a-e64ea9072812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_date(df):\n",
    "    # Convert 'Date' to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Extract year, month and day from 'Date'\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    # Sort DataFrame by date\n",
    "    df.sort_values('Date', inplace=True)\n",
    "\n",
    "    # Drop 'Date' column as it's no longer needed\n",
    "    df = df.drop('Date', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9361092e-52be-4644-b560-914211d91bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 67)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Group by 'State' column\n",
    "grouped = data.groupby('State')\n",
    "\n",
    "# Initialize a scaler and an encoder\n",
    "# scaler = MinMaxScaler()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "processed_dfs = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    # Separate target variable 'Anomaly'\n",
    "    target = group['Anomaly']\n",
    "    group = group.drop('Anomaly', axis=1)\n",
    "\n",
    "    # Normalize the group\n",
    "    numeric_cols = group.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # group[numeric_cols] = scaler.fit_transform(group[numeric_cols])\n",
    "    # group['Weekly_Sales'] = scaler.fit_transform(group[['Weekly_Sales']])\n",
    "    \n",
    "    # Select K best features\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selected_features = selector.fit_transform(group[numeric_cols], target)\n",
    "\n",
    "    # Concatenate selected numeric features and rejected non-numeric features\n",
    "    group = pd.concat([pd.DataFrame(selected_features, columns=numeric_cols, index=group.index), group.drop(columns=numeric_cols)], axis=1)\n",
    "    \n",
    "    \n",
    "    #Re-assigning 'Anomaly' to group\n",
    "    group['Anomaly'] = target\n",
    "    \n",
    "    # Create lagged features\n",
    "    for i in range(52, 0, -1):\n",
    "        group['Weekly_sales_lag_'+str(i)] = group['Weekly_Sales'].shift(i)\n",
    "\n",
    "    # # Handle any remaining NaN values\n",
    "    # group = group.dropna()\n",
    "        \n",
    "    # Append the result to the list\n",
    "    processed_dfs.append(group)\n",
    "\n",
    "# Concatenate all processed dfs\n",
    "data_preprocessed = pd.concat(processed_dfs)\n",
    "\n",
    "# Apply label encoding to 'State' column\n",
    "data_preprocessed['State'] = encoder.fit_transform(data['State'])\n",
    "\n",
    "data_preprocessed = preprocess_date(data_preprocessed)\n",
    "\n",
    "data_preprocessed.dropna(inplace=True)\n",
    "data_preprocessed.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44864c66-b75e-48b2-9cca-370cc88fc6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>State</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekly_sales_lag_7</th>\n",
       "      <th>Weekly_sales_lag_6</th>\n",
       "      <th>Weekly_sales_lag_5</th>\n",
       "      <th>Weekly_sales_lag_4</th>\n",
       "      <th>Weekly_sales_lag_3</th>\n",
       "      <th>Weekly_sales_lag_2</th>\n",
       "      <th>Weekly_sales_lag_1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>11421205.89</td>\n",
       "      <td>68.15</td>\n",
       "      <td>3.87</td>\n",
       "      <td>153.68</td>\n",
       "      <td>8.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12020871.82</td>\n",
       "      <td>9939602.47</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>10860567.44</td>\n",
       "      <td>11980295.30</td>\n",
       "      <td>10772361.35</td>\n",
       "      <td>11277845.75</td>\n",
       "      <td>10835830.55</td>\n",
       "      <td>10573794.19</td>\n",
       "      <td>10909456.28</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>8464546.97</td>\n",
       "      <td>41.87</td>\n",
       "      <td>3.33</td>\n",
       "      <td>173.06</td>\n",
       "      <td>8.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8704213.78</td>\n",
       "      <td>7483429.21</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>6760843.59</td>\n",
       "      <td>7253659.47</td>\n",
       "      <td>7089488.58</td>\n",
       "      <td>6845337.79</td>\n",
       "      <td>9119292.99</td>\n",
       "      <td>7180298.03</td>\n",
       "      <td>7960013.13</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>11715668.14</td>\n",
       "      <td>73.35</td>\n",
       "      <td>3.95</td>\n",
       "      <td>165.71</td>\n",
       "      <td>7.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12555119.33</td>\n",
       "      <td>9704251.54</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>11212867.28</td>\n",
       "      <td>10711004.76</td>\n",
       "      <td>11501434.14</td>\n",
       "      <td>11713115.39</td>\n",
       "      <td>11593601.41</td>\n",
       "      <td>11978527.86</td>\n",
       "      <td>11813627.25</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "87    11421205.89        68.15        3.87  153.68          8.36   \n",
       "227    8464546.97        41.87        3.33  173.06          8.01   \n",
       "417   11715668.14        73.35        3.95  165.71          7.31   \n",
       "\n",
       "     Holiday_Flag  Sales_Amount_Upper  Sales_Amount_Lower  State  month  ...  \\\n",
       "87            0.0         12020871.82          9939602.47      0      6  ...   \n",
       "227           0.0          8704213.78          7483429.21      4     12  ...   \n",
       "417           1.0         12555119.33          9704251.54      2      9  ...   \n",
       "\n",
       "     Weekly_sales_lag_7  Weekly_sales_lag_6  Weekly_sales_lag_5  \\\n",
       "87          10860567.44         11980295.30         10772361.35   \n",
       "227          6760843.59          7253659.47          7089488.58   \n",
       "417         11212867.28         10711004.76         11501434.14   \n",
       "\n",
       "     Weekly_sales_lag_4  Weekly_sales_lag_3  Weekly_sales_lag_2  \\\n",
       "87          11277845.75         10835830.55         10573794.19   \n",
       "227          6845337.79          9119292.99          7180298.03   \n",
       "417         11713115.39         11593601.41         11978527.86   \n",
       "\n",
       "     Weekly_sales_lag_1  Year  Month  Day  \n",
       "87          10909456.28  2022      6    3  \n",
       "227          7960013.13  2022     12   16  \n",
       "417         11813627.25  2023      9    8  \n",
       "\n",
       "[3 rows x 67 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b85afb7-a324-44a8-9deb-c3460af7f929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
       "       'Holiday_Flag', 'Sales_Amount_Upper', 'Sales_Amount_Lower', 'State',\n",
       "       'month', 'year', 'Anomaly', 'Weekly_sales_lag_52',\n",
       "       'Weekly_sales_lag_51', 'Weekly_sales_lag_50', 'Weekly_sales_lag_49',\n",
       "       'Weekly_sales_lag_48', 'Weekly_sales_lag_47', 'Weekly_sales_lag_46',\n",
       "       'Weekly_sales_lag_45', 'Weekly_sales_lag_44', 'Weekly_sales_lag_43',\n",
       "       'Weekly_sales_lag_42', 'Weekly_sales_lag_41', 'Weekly_sales_lag_40',\n",
       "       'Weekly_sales_lag_39', 'Weekly_sales_lag_38', 'Weekly_sales_lag_37',\n",
       "       'Weekly_sales_lag_36', 'Weekly_sales_lag_35', 'Weekly_sales_lag_34',\n",
       "       'Weekly_sales_lag_33', 'Weekly_sales_lag_32', 'Weekly_sales_lag_31',\n",
       "       'Weekly_sales_lag_30', 'Weekly_sales_lag_29', 'Weekly_sales_lag_28',\n",
       "       'Weekly_sales_lag_27', 'Weekly_sales_lag_26', 'Weekly_sales_lag_25',\n",
       "       'Weekly_sales_lag_24', 'Weekly_sales_lag_23', 'Weekly_sales_lag_22',\n",
       "       'Weekly_sales_lag_21', 'Weekly_sales_lag_20', 'Weekly_sales_lag_19',\n",
       "       'Weekly_sales_lag_18', 'Weekly_sales_lag_17', 'Weekly_sales_lag_16',\n",
       "       'Weekly_sales_lag_15', 'Weekly_sales_lag_14', 'Weekly_sales_lag_13',\n",
       "       'Weekly_sales_lag_12', 'Weekly_sales_lag_11', 'Weekly_sales_lag_10',\n",
       "       'Weekly_sales_lag_9', 'Weekly_sales_lag_8', 'Weekly_sales_lag_7',\n",
       "       'Weekly_sales_lag_6', 'Weekly_sales_lag_5', 'Weekly_sales_lag_4',\n",
       "       'Weekly_sales_lag_3', 'Weekly_sales_lag_2', 'Weekly_sales_lag_1',\n",
       "       'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7038af1-7c9e-4adb-9274-c96a07ea4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "# scaler = StandardScaler()\n",
    "# features = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'lag1', 'lag2', 'lag3', 'lag4']\n",
    "# data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Splitting the dataset respecting time series nature\n",
    "splitter = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in splitter.split(data_preprocessed):\n",
    "    train_data, test_data = data_preprocessed.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "train, validate, test = np.split(train_data, [int(.6*len(train_data)), int(.8*len(train_data))])\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train.drop(['Anomaly'], axis=1)\n",
    "y_train = train['Anomaly']\n",
    "X_validate = validate.drop(['Anomaly'], axis=1)\n",
    "y_validate = validate['Anomaly']\n",
    "X_test = test.drop(['Anomaly'], axis=1)\n",
    "y_test = test['Anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8b89b0-624f-45da-845d-9818964ed84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225, 66), (75, 66), (75, 66))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069124d-92bb-4e39-ace9-eea4950d2e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66943854-40aa-48ea-899c-dee1a7425204",
   "metadata": {},
   "source": [
    "# Train models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e9792e-e9c0-475e-ade2-25ca7fc401ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X, y, model_type=None):\n",
    "    if model_type == 'isolation_forest':\n",
    "        # Convert anomaly scores to binary labels\n",
    "        predictions = model.predict(X)\n",
    "        predictions = np.where(predictions == -1, 1, 0)  # Convert -1 to 1 (anomaly) and 1 to 0 (normal)\n",
    "    else:\n",
    "        predictions = model.predict(X)\n",
    "    # accuracy = accuracy_score(y, predictions)\n",
    "    # precision = precision_score(y, predictions, pos_label=1)\n",
    "    # recall = recall_score(y, predictions, pos_label=1)\n",
    "    # f1 = classification_report(y, predictions)#, pos_label=1)\n",
    "    return classification_report(y, predictions)\n",
    "    # accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4731b069-6242-486c-bc09-af7a808afc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anomaly\n",
       "0    55\n",
       "1    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de240e32-c1f9-407d-a49a-5a501bae004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest Metrics Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85        55\n",
      "           1       0.58      0.35      0.44        20\n",
      "\n",
      "    accuracy                           0.76        75\n",
      "   macro avg       0.69      0.63      0.64        75\n",
      "weighted avg       0.74      0.76      0.74        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=float(np.mean(y_train)))\n",
    "iso_forest.fit(X_train)\n",
    "iso_metrics = evaluate_model(iso_forest, X_validate, y_validate, 'isolation_forest')\n",
    "print(\"Isolation Forest Metrics Classification Report:\\n\", iso_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e03b423-3283-4694-bc70-6ad543fe9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.85        55\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.73        75\n",
      "   macro avg       0.37      0.50      0.42        75\n",
      "weighted avg       0.54      0.73      0.62        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_metrics = evaluate_model(rf_classifier, X_validate, y_validate)\n",
    "print(\"Random Forest Metrics Classification Report:\\n\", rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717146c5-6590-4229-80cd-23295f5acd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        55\n",
      "           1       1.00      0.05      0.10        20\n",
      "\n",
      "    accuracy                           0.75        75\n",
      "   macro avg       0.87      0.53      0.47        75\n",
      "weighted avg       0.81      0.75      0.65        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_metrics = evaluate_model(xgb_classifier, X_validate, y_validate)\n",
    "print(\"XGBoost Metrics Classification Report:\\n\", xgb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c76b56-9f63-46fe-be4a-a67bdcfea9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66dcb96-b5d8-4fa9-a6ca-b954d7ed96ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
