{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1762cff4-ca8f-4157-9ce5-1a12654538ee",
   "metadata": {},
   "source": [
    "*Training an XGBoost model on the Walmart Daily data to identify anomalies in it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014099b7-0b55-474d-bf58-f2ea3e39baba",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0839c5-2471-4e29-948e-66c0a2531c49",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c0facc-e467-4ac1-b768-64a33a515005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import datetime \n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcce45d-b12b-48d6-979d-44fbbbdd0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'C:\\\\Geeta\\\\learning\\\\projects\\\\AnomalyDetectionSXM\\\\Notebooks\\\\Datasets\\\\Pipeline'\n",
    "dataset_name = 'Walmart_Weekly'\n",
    "train_file = base_folder + '/train/' + dataset_name +'_train.csv'\n",
    "inference_file = base_folder + '/inference/' + dataset_name +'_inference.csv'\n",
    "\n",
    "current_date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_file_path = base_folder + '/xgboostADmodel_' + dataset_name + current_date_time + '.pkl'\n",
    "\n",
    "inference_results_file = base_folder + '/inference/' + dataset_name +'_inference_results.csv'\n",
    "\n",
    "target = 'Weekly_Sales'\n",
    "# target = 'Daily_Sales'\n",
    "\n",
    "# Group the data by 'State' and perform lag shifting within each group\n",
    "groupby_cols=['State'] \n",
    "\n",
    "# Define lag columns to consider\n",
    "lag_columns = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "\n",
    "# Define the lag values to be used\n",
    "lags = [1, 2, 4]  # Example lag values of 1 week and 2 weeks\n",
    "\n",
    "state_encoder = 'label_encoder_State.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67beef6d-740c-4094-8b1f-96fa3c201a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28797162-5f06-47f0-8a0e-6d6952b1e53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Geeta\\\\learning\\\\projects\\\\AnomalyDetectionSXM\\\\Notebooks\\\\Datasets\\\\Pipeline/train/Walmart_Weekly_train.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853fd3a-b9ef-4d6b-8e8b-46b94d2e999d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e96464-953a-401e-a6fb-8f5d32a1bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data_path, sheet_name = '', usecols = None):\n",
    "    df = pd.DataFrame()\n",
    "    if data_path.split('.')[-1] == 'xlsx':\n",
    "        if sheet_name:\n",
    "            df = pd.read_excel(data_path, sheet_name=sheet_name, usecols=usecols)\n",
    "        else:\n",
    "            df = pd.read_excel(data_path, usecols=usecols)\n",
    "        print(\"Shape of the data in file {} is {}\".format(data_path, df.shape))\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(data_path)\n",
    "            print(\"Shape of the data in file {} is {}\".format(data_path, df.shape))\n",
    "            if df.shape[0] == 0:\n",
    "                print(\"No data in file {}\".format(data_path))\n",
    "        except Exception as e:\n",
    "            print(\"Issue while reading data at {} \\n{}\".format(data_path, e))\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_date_col(dataframe, date_col):\n",
    "    dataframe[date_col] = pd.to_datetime(dataframe[date_col], infer_datetime_format=True) #.fillna(pd.to_datetime(df['Date'], format='%d/%m/%y', errors='coerce'))\n",
    "    # dataframe[date_col] = pd.to_datetime(dataframe[date_col], format='%Y-%m-%d', errors='coerce')#.fillna(pd.to_datetime(df['Date'], format='%d/%m/%y', errors='coerce'))\n",
    "    # Convert all dates to 'mm-dd-yyyy' format\n",
    "    # dataframe[date_col] = dataframe[date_col].dt.strftime('%Y-%m-%d')\n",
    "    return dataframe\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e54ea7e-d096-41d9-9275-6a894a37eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data in file C:\\Geeta\\learning\\projects\\AnomalyDetectionSXM\\Notebooks\\Datasets\\Pipeline/train/Walmart_Weekly_train.csv is (710, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-05</td>\n",
       "      <td>10397622.73</td>\n",
       "      <td>37.20</td>\n",
       "      <td>2.58</td>\n",
       "      <td>200.61</td>\n",
       "      <td>7.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11573691.83</td>\n",
       "      <td>9513064.59</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-12</td>\n",
       "      <td>10378496.65</td>\n",
       "      <td>36.72</td>\n",
       "      <td>2.55</td>\n",
       "      <td>200.74</td>\n",
       "      <td>7.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11032180.28</td>\n",
       "      <td>8971553.04</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>10060556.61</td>\n",
       "      <td>39.70</td>\n",
       "      <td>2.52</td>\n",
       "      <td>200.79</td>\n",
       "      <td>7.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10763335.98</td>\n",
       "      <td>8702708.74</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "0 2021-02-05   10397622.73        37.20        2.58  200.61          7.55   \n",
       "1 2021-02-12   10378496.65        36.72        2.55  200.74          7.55   \n",
       "2 2021-02-19   10060556.61        39.70        2.52  200.79          7.55   \n",
       "\n",
       "   Holiday_Flag  Anomaly  Sales_Amount_Upper  Sales_Amount_Lower    State  \n",
       "0             0        0         11573691.83          9513064.59  Florida  \n",
       "1             1        0         11032180.28          8971553.04  Florida  \n",
       "2             0        0         10763335.98          8702708.74  Florida  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from csv or excel, sheet_name is the sheet in excel that contians data \n",
    "data = read(data_path, sheet_name= 'RAW')\n",
    "data = standardize_date_col(data, 'Date')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924ccfcb-93b7-479d-bfb7-9b933a6c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by the 'Date' column in ascending order\n",
    "data = data.sort_values('Date')\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceac9604-a065-4943-a29b-98c11ef795eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI',\n",
       "       'Unemployment', 'Holiday_Flag', 'Anomaly', 'Sales_Amount_Upper',\n",
       "       'Sales_Amount_Lower', 'State'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3ae2d-beb3-4fae-8b01-b7ff75b4dbd9",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d22ff78-b5e9-4c63-ac33-543c00198c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d40b920-c36c-4f3b-bf70-b6dc5f48f3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710, 271)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lag_columns(data, groupby_cols, lag_columns):\n",
    "    # Group the data by 'State' and perform lag shifting within each group\n",
    "    grouped = data.groupby(groupby_cols)\n",
    "    \n",
    "    \n",
    "    # Create lag features within each group\n",
    "    for lag in lags:\n",
    "        for col in lag_columns:\n",
    "            data[f'{col}_lag_{lag}'] = grouped[col].shift(lag)\n",
    "            data[f'{col}_lag_{lag}'] = data[f'{col}_lag_{lag}'].bfill()\n",
    "    print(\"Before dropping NAs:\", data.shape)\n",
    "    data.dropna(inplace=True)\n",
    "    print(\"After dropping NAs:\", data.shape)\n",
    "    return data\n",
    "\n",
    "def get_lag_Weekly_Sales(data, groupby_cols, lag_columns):\n",
    "    # Group the data by 'State' and perform lag shifting within each group\n",
    "    group = data.groupby(groupby_cols)\n",
    "    \n",
    "    # Create lagged features\n",
    "    for col in lag_columns:\n",
    "        for i in range(52, 0, -1):\n",
    "            data[col + '_' +str(i)] = group[col].shift(i)\n",
    "    return data\n",
    "        \n",
    "    \n",
    "# data = get_lag_columns(data, groupby_cols, lag_columns)\n",
    "data = get_lag_Weekly_Sales(data, groupby_cols, lag_columns)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecdb7066-6ed0-4882-8070-b8b6fae5bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 271)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de41a44c-4be1-4fb1-8335-6a737e12af75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI',\n",
       "       'Unemployment', 'Holiday_Flag', 'Anomaly', 'Sales_Amount_Upper',\n",
       "       'Sales_Amount_Lower',\n",
       "       ...\n",
       "       'Unemployment_10', 'Unemployment_9', 'Unemployment_8', 'Unemployment_7',\n",
       "       'Unemployment_6', 'Unemployment_5', 'Unemployment_4', 'Unemployment_3',\n",
       "       'Unemployment_2', 'Unemployment_1'],\n",
       "      dtype='object', length=271)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95597b8a-2b8d-475d-96c2-ace9faf15c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_features(data, date_col):\n",
    "    def week_of_month(date):\n",
    "        # Get the first day of the month\n",
    "        first_day = date.replace(day=1)\n",
    "        # Calculate the adjusted day of the week (0=Monday, ..., 6=Sunday)\n",
    "        adjusted_dom = (first_day.weekday() + 1) % 7\n",
    "        # Calculate the week of the month\n",
    "        week_of_month = (date.day + adjusted_dom - 1) // 7 + 1\n",
    "        return week_of_month\n",
    "    \n",
    "    data['Week_Of_Month'] = data[date_col].map(week_of_month)\n",
    "    # Create time-based features\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    # data['Day_of_Week'] = data['Date'].dt.dayofweek  # Day of the week (0: Monday, 1: Tuesday, ..., 6: Sunday)\n",
    "    data['Month'] = data[date_col].dt.month  # Month of the year (1 to 12)\n",
    "    data['Quarter'] = data[date_col].dt.quarter  # Quarter of the year (1 to 4)\n",
    "    data['Year'] = data[date_col].dt.year  # Year\n",
    "    return data\n",
    "\n",
    "data = get_date_features(data, 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07b9bed7-8c93-4b8e-a943-563c438cdacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>...</th>\n",
       "      <th>Unemployment_6</th>\n",
       "      <th>Unemployment_5</th>\n",
       "      <th>Unemployment_4</th>\n",
       "      <th>Unemployment_3</th>\n",
       "      <th>Unemployment_2</th>\n",
       "      <th>Unemployment_1</th>\n",
       "      <th>Week_Of_Month</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>11153431.69</td>\n",
       "      <td>47.82</td>\n",
       "      <td>3.56</td>\n",
       "      <td>155.31</td>\n",
       "      <td>8.01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13506883.25</td>\n",
       "      <td>11425613.90</td>\n",
       "      <td>...</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.01</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>9908450.93</td>\n",
       "      <td>65.74</td>\n",
       "      <td>3.83</td>\n",
       "      <td>210.06</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11640852.67</td>\n",
       "      <td>9580225.42</td>\n",
       "      <td>...</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>10291510.29</td>\n",
       "      <td>53.28</td>\n",
       "      <td>3.63</td>\n",
       "      <td>209.75</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11368810.47</td>\n",
       "      <td>9308183.23</td>\n",
       "      <td>...</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>9664462.49</td>\n",
       "      <td>58.94</td>\n",
       "      <td>3.77</td>\n",
       "      <td>209.97</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11291870.79</td>\n",
       "      <td>9231243.55</td>\n",
       "      <td>...</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>11421205.89</td>\n",
       "      <td>68.15</td>\n",
       "      <td>3.87</td>\n",
       "      <td>153.68</td>\n",
       "      <td>8.36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12020871.82</td>\n",
       "      <td>9939602.47</td>\n",
       "      <td>...</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.36</td>\n",
       "      <td>8.36</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "466 2022-11-18   11153431.69        47.82        3.56  155.31          8.01   \n",
       "562 2023-03-31    9908450.93        65.74        3.83  210.06          6.41   \n",
       "549 2023-03-10   10291510.29        53.28        3.63  209.75          6.41   \n",
       "556 2023-03-24    9664462.49        58.94        3.77  209.97          6.41   \n",
       "349 2022-06-03   11421205.89        68.15        3.87  153.68          8.36   \n",
       "\n",
       "     Holiday_Flag  Anomaly  Sales_Amount_Upper  Sales_Amount_Lower  ...  \\\n",
       "466             0        1         13506883.25         11425613.90  ...   \n",
       "562             0        0         11640852.67          9580225.42  ...   \n",
       "549             0        0         11368810.47          9308183.23  ...   \n",
       "556             0        0         11291870.79          9231243.55  ...   \n",
       "349             0        0         12020871.82          9939602.47  ...   \n",
       "\n",
       "    Unemployment_6  Unemployment_5  Unemployment_4  Unemployment_3  \\\n",
       "466           8.01            8.01            8.01            8.01   \n",
       "562           6.41            6.41            6.41            6.41   \n",
       "549           6.41            6.41            6.41            6.41   \n",
       "556           6.41            6.41            6.41            6.41   \n",
       "349           8.36            8.36            8.36            8.36   \n",
       "\n",
       "     Unemployment_2  Unemployment_1  Week_Of_Month  Month  Quarter  Year  \n",
       "466            8.01            8.01              3     11        4  2022  \n",
       "562            6.41            6.41              5      3        1  2023  \n",
       "549            6.41            6.41              2      3        1  2023  \n",
       "556            6.41            6.41              4      3        1  2023  \n",
       "349            8.36            8.36              1      6        2  2022  \n",
       "\n",
       "[5 rows x 275 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7e37d7-055d-48ad-aa77-33e054b044c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder_State.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode categorical columns\n",
    "encoder = LabelEncoder()\n",
    "data['State'] = encoder.fit_transform(data['State'])\n",
    "\n",
    "# Save the trained label encoder\n",
    "joblib.dump(encoder, state_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f00c62-0201-46d4-bd44-8bb82488922f",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3deee33f-2687-4c0c-b694-bacfff9ae14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "# data.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5020ed5-a61e-4f86-8bed-73a5ebaf8438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>...</th>\n",
       "      <th>Unemployment_6</th>\n",
       "      <th>Unemployment_5</th>\n",
       "      <th>Unemployment_4</th>\n",
       "      <th>Unemployment_3</th>\n",
       "      <th>Unemployment_2</th>\n",
       "      <th>Unemployment_1</th>\n",
       "      <th>Week_Of_Month</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>7420671.08</td>\n",
       "      <td>66.94</td>\n",
       "      <td>3.50</td>\n",
       "      <td>167.79</td>\n",
       "      <td>8.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7688144.87</td>\n",
       "      <td>6336700.40</td>\n",
       "      <td>...</td>\n",
       "      <td>9.29</td>\n",
       "      <td>9.29</td>\n",
       "      <td>9.29</td>\n",
       "      <td>9.29</td>\n",
       "      <td>9.29</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>9553921.17</td>\n",
       "      <td>73.17</td>\n",
       "      <td>3.55</td>\n",
       "      <td>205.24</td>\n",
       "      <td>7.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10296654.30</td>\n",
       "      <td>8236027.05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>11491173.47</td>\n",
       "      <td>33.07</td>\n",
       "      <td>3.74</td>\n",
       "      <td>164.23</td>\n",
       "      <td>7.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12269936.54</td>\n",
       "      <td>9419068.75</td>\n",
       "      <td>...</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>11284939.90</td>\n",
       "      <td>42.37</td>\n",
       "      <td>3.46</td>\n",
       "      <td>209.22</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11179403.79</td>\n",
       "      <td>9118776.54</td>\n",
       "      <td>...</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.41</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>9710842.80</td>\n",
       "      <td>36.15</td>\n",
       "      <td>3.54</td>\n",
       "      <td>163.71</td>\n",
       "      <td>7.20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12943328.33</td>\n",
       "      <td>10092460.55</td>\n",
       "      <td>...</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "438 2022-10-07    7420671.08        66.94        3.50  167.79          8.92   \n",
       "416 2022-09-09    9553921.17        73.17        3.55  205.24          7.10   \n",
       "532 2023-02-17   11491173.47        33.07        3.74  164.23          7.20   \n",
       "534 2023-02-17   11284939.90        42.37        3.46  209.22          6.41   \n",
       "507 2023-01-13    9710842.80        36.15        3.54  163.71          7.20   \n",
       "\n",
       "     Holiday_Flag  Anomaly  Sales_Amount_Upper  Sales_Amount_Lower  ...  \\\n",
       "438             0        0          7688144.87          6336700.40  ...   \n",
       "416             1        0         10296654.30          8236027.05  ...   \n",
       "532             0        0         12269936.54          9419068.75  ...   \n",
       "534             0        1         11179403.79          9118776.54  ...   \n",
       "507             0        1         12943328.33         10092460.55  ...   \n",
       "\n",
       "     Unemployment_6  Unemployment_5  Unemployment_4  Unemployment_3  \\\n",
       "438            9.29            9.29            9.29            9.29   \n",
       "416            7.10            7.10            7.10            7.10   \n",
       "532            7.20            7.20            7.20            7.20   \n",
       "534            6.41            6.41            6.41            6.41   \n",
       "507            7.28            7.28            7.28            7.28   \n",
       "\n",
       "     Unemployment_2  Unemployment_1  Week_Of_Month  Month  Quarter  Year  \n",
       "438            9.29            9.29              2     10        4  2022  \n",
       "416            7.10            7.10              2      9        3  2022  \n",
       "532            7.20            7.20              3      2        1  2023  \n",
       "534            6.41            6.41              3      2        1  2023  \n",
       "507            7.28            7.20              2      1        1  2023  \n",
       "\n",
       "[5 rows x 275 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51679a-f5cb-4a00-89d8-859f1fe6755d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50b62622-e147-4d34-b59f-14feefcc7f51",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "290e89fd-9451-40d2-a171-d8979e7dd67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 64, 135, 450, 450)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(np.floor(data.shape[0]*0.8*0.7))\n",
    "val_size = int(np.floor(data.shape[0]*0.7) - train_size)\n",
    "test_size = int(data.shape[0] - train_size - val_size)\n",
    "\n",
    "train_size, val_size, test_size, train_size + val_size + test_size, data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b1a44f-1e90-4866-b67d-a289b58681ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 275),\n",
       " (64, 275),\n",
       " (135, 275),\n",
       " Anomaly\n",
       " 0    131\n",
       " 1      4\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data.iloc[:train_size]\n",
    "val_data = data.iloc[train_size:train_size+val_size]\n",
    "test_data = data.iloc[:test_size]\n",
    "\n",
    "train_data.shape, val_data.shape, test_data.shape, test_data['Anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f987ae60-f3a9-46c3-96a6-70ef4a35797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 273), (64, 273), (115, 273), (20, 273))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_Xy(data, drop_cols, target_col):\n",
    "    X = data.drop(drop_cols, axis=1)\n",
    "    y = data['Anomaly']\n",
    "    return X,y\n",
    "\n",
    "\n",
    "    \n",
    "drop_cols = ['Date', 'Anomaly']  #, 'Sales_Amount_Upper', 'Sales_Amount_Lower'\n",
    "\n",
    "X_train, y_train = get_Xy(train_data, drop_cols, 'Anomaly')\n",
    "X_val, y_val = get_Xy(val_data, drop_cols, 'Anomaly')\n",
    "X_test, y_test = get_Xy(test_data, drop_cols, 'Anomaly')\n",
    "X_test2, y_test2 = X_test.iloc[-20:], y_test[-20:]\n",
    "X_test, y_test = X_test.iloc[:-20], y_test[:-20]\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "984f70eb-a019-4b7a-ade8-e85d5565490a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Sales_Amount_Upper</th>\n",
       "      <th>Sales_Amount_Lower</th>\n",
       "      <th>State</th>\n",
       "      <th>Weekly_Sales_52</th>\n",
       "      <th>...</th>\n",
       "      <th>Unemployment_6</th>\n",
       "      <th>Unemployment_5</th>\n",
       "      <th>Unemployment_4</th>\n",
       "      <th>Unemployment_3</th>\n",
       "      <th>Unemployment_2</th>\n",
       "      <th>Unemployment_1</th>\n",
       "      <th>Week_Of_Month</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>7444909.07</td>\n",
       "      <td>34.24</td>\n",
       "      <td>3.13</td>\n",
       "      <td>164.55</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0</td>\n",
       "      <td>7850346.28</td>\n",
       "      <td>6498901.81</td>\n",
       "      <td>3</td>\n",
       "      <td>8161946.14</td>\n",
       "      <td>...</td>\n",
       "      <td>9.67</td>\n",
       "      <td>9.67</td>\n",
       "      <td>9.49</td>\n",
       "      <td>9.49</td>\n",
       "      <td>9.49</td>\n",
       "      <td>9.49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>6705436.37</td>\n",
       "      <td>32.87</td>\n",
       "      <td>3.12</td>\n",
       "      <td>168.75</td>\n",
       "      <td>8.68</td>\n",
       "      <td>0</td>\n",
       "      <td>6946379.76</td>\n",
       "      <td>5725595.20</td>\n",
       "      <td>4</td>\n",
       "      <td>6575770.40</td>\n",
       "      <td>...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>8.85</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.68</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>9985425.58</td>\n",
       "      <td>33.06</td>\n",
       "      <td>2.99</td>\n",
       "      <td>202.16</td>\n",
       "      <td>7.20</td>\n",
       "      <td>0</td>\n",
       "      <td>10587907.70</td>\n",
       "      <td>8527280.46</td>\n",
       "      <td>1</td>\n",
       "      <td>10397622.73</td>\n",
       "      <td>...</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weekly_Sales  Temperature  Fuel_Price     CPI  Unemployment  \\\n",
       "260    7444909.07        34.24        3.13  164.55          9.49   \n",
       "261    6705436.37        32.87        3.12  168.75          8.68   \n",
       "262    9985425.58        33.06        2.99  202.16          7.20   \n",
       "\n",
       "     Holiday_Flag  Sales_Amount_Upper  Sales_Amount_Lower  State  \\\n",
       "260             0          7850346.28          6498901.81      3   \n",
       "261             0          6946379.76          5725595.20      4   \n",
       "262             0         10587907.70          8527280.46      1   \n",
       "\n",
       "     Weekly_Sales_52  ...  Unemployment_6  Unemployment_5  Unemployment_4  \\\n",
       "260       8161946.14  ...            9.67            9.67            9.49   \n",
       "261       6575770.40  ...            8.85            8.85            8.68   \n",
       "262      10397622.73  ...            7.40            7.40            7.20   \n",
       "\n",
       "     Unemployment_3  Unemployment_2  Unemployment_1  Week_Of_Month  Month  \\\n",
       "260            9.49            9.49            9.49              1      2   \n",
       "261            8.68            8.68            8.68              1      2   \n",
       "262            7.20            7.20            7.20              1      2   \n",
       "\n",
       "     Quarter  Year  \n",
       "260        1  2022  \n",
       "261        1  2022  \n",
       "262        1  2022  \n",
       "\n",
       "[3 rows x 273 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2dd33a7-daa1-46bd-8012-50aab1121573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
       "       'Holiday_Flag', 'Sales_Amount_Upper', 'Sales_Amount_Lower', 'State',\n",
       "       'Weekly_Sales_52',\n",
       "       ...\n",
       "       'Unemployment_6', 'Unemployment_5', 'Unemployment_4', 'Unemployment_3',\n",
       "       'Unemployment_2', 'Unemployment_1', 'Week_Of_Month', 'Month', 'Quarter',\n",
       "       'Year'],\n",
       "      dtype='object', length=273)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331e24e-e98f-48fe-bd84-bdad74f9ee53",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e367f5b-659e-421c-b932-ae33f2bd99ff",
   "metadata": {},
   "source": [
    "### Add more features:\n",
    "* rolling statistics for weekly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34c0449b-c819-4503-bf88-06e6f55ff286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data[f'{col}_lag_{lag}'] = data[f'{col}_lag_{lag}'].bfill()\n",
    "# # Calculate rolling statistics\n",
    "# window = 4  # Window size for rolling statistics\n",
    "# data['Rolling_Mean_Weekly_Sales'] = data['Weekly_Sales'].rolling(window=window).mean()  # Rolling mean of weekly sales\n",
    "# data['Rolling_Mean_Weekly_Sales'] = data['Rolling_Mean_Weekly_Sales'].bfill() # Backfill NAs, TODO: Change this to see if results improve\n",
    "\n",
    "# data['Rolling_Std_Weekly_Sales'] = data['Weekly_Sales'].rolling(window=window).std()  # Rolling standard deviation of weekly sales\n",
    "# data['Rolling_Std_Weekly_Sales'] = data['Rolling_Std_Weekly_Sales'].bfill()\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cccf01-fdc0-4ec2-9757-bcb9a2181466",
   "metadata": {},
   "source": [
    "### Resampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ec3116-4693-43c2-b54b-76bc3a5d805f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# # Define the resampling strategy using a pipeline\n",
    "# resample_pipeline = make_pipeline(SMOTE(random_state=42), RandomUnderSampler(random_state=42))\n",
    "\n",
    "# # Apply the resampling strategy to the training data\n",
    "# X_resampled, y_resampled = resample_pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923c564-fad3-47ac-bb3f-0dcd8358ecfe",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8beed06-a6c8-42f6-88d6-9c848e03a89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results:\n",
      "{'mean_fit_time': array([0.4366724 , 0.45224438, 0.71659408, 0.75938945, 0.46317549,\n",
      "       0.49464908, 0.81478834, 0.91799836, 0.47080965, 0.51360831,\n",
      "       0.75159535, 0.77537436, 0.44454899, 0.46144567, 0.7331737 ,\n",
      "       0.72161007, 0.51987977, 0.5476645 , 0.71962919, 0.7726953 ,\n",
      "       0.45983701, 0.4881989 , 0.72388659, 0.75936656, 0.43595705,\n",
      "       0.45358233, 0.72016311, 0.72081623, 0.43546481, 0.4738338 ,\n",
      "       0.71401262, 0.72120447, 0.44974961, 0.47253065, 0.71955295,\n",
      "       0.7330193 , 0.42938695, 0.44912157, 0.70454116, 0.70615296,\n",
      "       0.44814191, 0.45866804, 0.68513889, 0.73026466, 0.45199256,\n",
      "       0.46545   , 0.69870267, 0.69842105, 0.50203819, 0.52455797,\n",
      "       0.79374781, 0.82831573, 0.52136774, 0.55937233, 0.81590977,\n",
      "       0.86355858, 0.50074782, 0.57075644, 0.79854522, 0.86159039,\n",
      "       0.47962141, 0.49799161, 0.76699896, 0.78965559, 0.48068833,\n",
      "       0.53090053, 0.75929346, 0.80938578, 0.47860675, 0.52487092,\n",
      "       0.7694097 , 0.81715007, 0.46242294, 0.49370537, 0.75364046,\n",
      "       0.76915221, 0.47878413, 0.49219651, 0.7537457 , 0.77211485,\n",
      "       0.47068768, 0.49577632, 0.76925039, 0.79042001, 0.48123226,\n",
      "       0.49027572, 0.74797573, 0.74002519, 0.45829825, 0.49942808,\n",
      "       0.7244966 , 0.78906689, 0.4793745 , 0.512713  , 0.73707385,\n",
      "       0.79417992]), 'std_fit_time': array([0.03475368, 0.01896322, 0.04797959, 0.03335282, 0.01993654,\n",
      "       0.02890593, 0.18540294, 0.16339251, 0.0289345 , 0.03768167,\n",
      "       0.03836522, 0.03562706, 0.01673458, 0.02817397, 0.03490618,\n",
      "       0.04673011, 0.14449665, 0.16487835, 0.04900169, 0.05803191,\n",
      "       0.01254515, 0.03382158, 0.03729185, 0.04938085, 0.03613134,\n",
      "       0.02633948, 0.03224631, 0.03195956, 0.02362949, 0.03622859,\n",
      "       0.02794596, 0.04048974, 0.02938016, 0.0315778 , 0.03999881,\n",
      "       0.0478572 , 0.02239722, 0.0166682 , 0.02837223, 0.02521064,\n",
      "       0.01530427, 0.03097243, 0.05098023, 0.03841791, 0.02600062,\n",
      "       0.02798335, 0.03693293, 0.04749182, 0.03398211, 0.03183576,\n",
      "       0.04930177, 0.0609801 , 0.02988676, 0.05265682, 0.04416134,\n",
      "       0.0724155 , 0.04030745, 0.05010073, 0.06500657, 0.06936206,\n",
      "       0.02842111, 0.03163011, 0.05594935, 0.05695445, 0.02252549,\n",
      "       0.03201051, 0.05458776, 0.05990784, 0.0288782 , 0.04123049,\n",
      "       0.05561455, 0.07114115, 0.01440579, 0.02916172, 0.04468367,\n",
      "       0.03533377, 0.02914991, 0.02963984, 0.0404758 , 0.04902242,\n",
      "       0.03370277, 0.0469412 , 0.06461263, 0.0540319 , 0.03070442,\n",
      "       0.03368339, 0.01766369, 0.03194644, 0.02358139, 0.04426584,\n",
      "       0.03267401, 0.06925404, 0.04463163, 0.05470662, 0.03465526,\n",
      "       0.05280256]), 'mean_score_time': array([0.08083172, 0.08708344, 0.08934283, 0.08116832, 0.08969617,\n",
      "       0.08846054, 0.08519335, 0.08175831, 0.08707829, 0.08804107,\n",
      "       0.08868775, 0.0896656 , 0.09103293, 0.08316035, 0.08692799,\n",
      "       0.08587189, 0.08062148, 0.08753877, 0.08790622, 0.09074259,\n",
      "       0.08170261, 0.08778043, 0.07977791, 0.09103198, 0.09105778,\n",
      "       0.08667626, 0.08778839, 0.09496698, 0.09095449, 0.08364029,\n",
      "       0.08948445, 0.08478746, 0.09405818, 0.08540359, 0.08992796,\n",
      "       0.08363061, 0.09043312, 0.09573002, 0.09410849, 0.08796296,\n",
      "       0.09223924, 0.09533281, 0.08839641, 0.09042411, 0.08275461,\n",
      "       0.0920332 , 0.09084277, 0.0834806 , 0.09103413, 0.08777122,\n",
      "       0.08610535, 0.09357023, 0.08947892, 0.09252276, 0.08478646,\n",
      "       0.08793235, 0.09274416, 0.0854753 , 0.09175997, 0.09415889,\n",
      "       0.09382358, 0.08980079, 0.08478522, 0.09020486, 0.08943172,\n",
      "       0.08620539, 0.08432212, 0.09103894, 0.08201537, 0.09223514,\n",
      "       0.09342237, 0.08696446, 0.09188004, 0.09233174, 0.09195843,\n",
      "       0.09189668, 0.09242225, 0.0909153 , 0.09540558, 0.09261084,\n",
      "       0.08470659, 0.09078894, 0.09268856, 0.09688134, 0.09369764,\n",
      "       0.09306407, 0.09641051, 0.08775978, 0.0931006 , 0.09908714,\n",
      "       0.0928091 , 0.09913998, 0.09522448, 0.09249921, 0.09240813,\n",
      "       0.09344907]), 'std_score_time': array([7.41004073e-03, 7.27487894e-03, 5.79385161e-03, 1.08051362e-02,\n",
      "       7.69255435e-03, 1.28102226e-02, 9.57467770e-03, 1.19404220e-02,\n",
      "       7.17878216e-03, 9.76551449e-03, 6.78888214e-03, 6.35092940e-03,\n",
      "       6.25226544e-03, 6.22254523e-03, 7.20388231e-03, 6.34322468e-03,\n",
      "       1.11875632e-02, 7.02070796e-03, 7.65471598e-03, 6.76317684e-03,\n",
      "       6.22825560e-03, 7.81043164e-03, 1.19112426e-02, 6.24499515e-03,\n",
      "       6.25910542e-03, 9.40590811e-03, 7.55840917e-03, 1.32107305e-03,\n",
      "       6.40974304e-03, 6.60306810e-03, 5.78757589e-03, 7.82227255e-03,\n",
      "       2.02377836e-04, 6.30278071e-03, 5.34675217e-03, 8.56307338e-03,\n",
      "       7.45463589e-03, 2.44612359e-03, 3.36380365e-04, 7.59158842e-03,\n",
      "       7.23173644e-03, 2.35500472e-03, 8.10429788e-03, 6.06055635e-03,\n",
      "       6.09013423e-03, 1.23015220e-02, 8.01399715e-03, 6.39970077e-03,\n",
      "       6.25560310e-03, 5.77873879e-03, 7.00430626e-03, 9.28329682e-03,\n",
      "       6.32184738e-03, 7.29115239e-03, 7.65576586e-03, 5.12418420e-03,\n",
      "       2.83529820e-03, 7.19953838e-03, 4.80260925e-03, 4.51109122e-06,\n",
      "       5.00837769e-04, 6.07104108e-03, 1.24985580e-02, 6.06062176e-03,\n",
      "       6.26916499e-03, 1.03496459e-02, 6.57964492e-03, 6.25431651e-03,\n",
      "       6.96737858e-03, 7.23408502e-03, 1.47900810e-03, 1.18797866e-02,\n",
      "       6.87248199e-03, 4.70339605e-03, 7.02105898e-03, 5.85016793e-03,\n",
      "       7.15373665e-03, 6.19333453e-03, 2.86043689e-03, 4.22895121e-03,\n",
      "       1.41439300e-02, 6.61017238e-03, 7.11215776e-03, 2.07112817e-03,\n",
      "       5.53590022e-03, 3.99509157e-03, 3.84196915e-03, 6.05647195e-03,\n",
      "       1.62625874e-02, 6.29439957e-03, 5.39901641e-03, 7.37924743e-03,\n",
      "       1.17884378e-03, 7.46640103e-03, 4.60354246e-03, 1.87911370e-03]), 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
      "                   0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
      "                   0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[3, 3, 3, 3, 5, 5, 5, 5, 7, 7, 7, 7, 3, 3, 3, 3, 5, 5,\n",
      "                   5, 5, 7, 7, 7, 7, 3, 3, 3, 3, 5, 5, 5, 5, 7, 7, 7, 7,\n",
      "                   3, 3, 3, 3, 5, 5, 5, 5, 7, 7, 7, 7, 3, 3, 3, 3, 5, 5,\n",
      "                   5, 5, 7, 7, 7, 7, 3, 3, 3, 3, 5, 5, 5, 5, 7, 7, 7, 7,\n",
      "                   3, 3, 3, 3, 5, 5, 5, 5, 7, 7, 7, 7, 3, 3, 3, 3, 5, 5,\n",
      "                   5, 5, 7, 7, 7, 7],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[100, 100, 200, 200, 100, 100, 200, 200, 100, 100, 200,\n",
      "                   200, 100, 100, 200, 200, 100, 100, 200, 200, 100, 100,\n",
      "                   200, 200, 100, 100, 200, 200, 100, 100, 200, 200, 100,\n",
      "                   100, 200, 200, 100, 100, 200, 200, 100, 100, 200, 200,\n",
      "                   100, 100, 200, 200, 100, 100, 200, 200, 100, 100, 200,\n",
      "                   200, 100, 100, 200, 200, 100, 100, 200, 200, 100, 100,\n",
      "                   200, 200, 100, 100, 200, 200, 100, 100, 200, 200, 100,\n",
      "                   100, 200, 200, 100, 100, 200, 200, 100, 100, 200, 200,\n",
      "                   100, 100, 200, 200, 100, 100, 200, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_subsample': masked_array(data=[0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
      "                   0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
      "                   0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
      "                   0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
      "                   0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
      "                   0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
      "                   0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
      "                   0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
      "                   0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.5, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}], 'split0_test_score': array([0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451,\n",
      "       0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451, 0.8627451]), 'split1_test_score': array([0.9 , 0.9 , 0.9 , 0.9 , 0.88, 0.9 , 0.9 , 0.9 , 0.88, 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.88, 0.9 , 0.88, 0.9 ,\n",
      "       0.88, 0.9 , 0.88, 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.88, 0.9 , 0.88,\n",
      "       0.9 , 0.88, 0.9 , 0.88, 0.88, 0.9 , 0.88, 0.9 , 0.88, 0.9 , 0.88,\n",
      "       0.9 , 0.88, 0.9 , 0.88, 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.88, 0.9 ,\n",
      "       0.88, 0.88, 0.88, 0.9 , 0.88, 0.88, 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.88, 0.9 , 0.88, 0.9 ,\n",
      "       0.88, 0.9 , 0.88, 0.9 , 0.88, 0.9 , 0.88, 0.9 ]), 'split2_test_score': array([0.94, 0.94, 0.94, 0.94, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94,\n",
      "       0.96, 0.94, 0.94, 0.96, 0.96, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94,\n",
      "       0.94, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96, 0.94, 0.96, 0.94, 0.96,\n",
      "       0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94,\n",
      "       0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96,\n",
      "       0.96, 0.96, 0.96, 0.96, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.94,\n",
      "       0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96,\n",
      "       0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96, 0.96,\n",
      "       0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]), 'split3_test_score': array([0.98, 0.98, 0.98, 1.  , 0.98, 1.  , 1.  , 1.  , 0.98, 1.  , 1.  ,\n",
      "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.98,\n",
      "       1.  , 0.98, 1.  , 1.  , 1.  , 1.  , 1.  , 0.98, 1.  , 0.98, 1.  ,\n",
      "       0.98, 1.  , 0.98, 1.  , 1.  , 1.  , 1.  , 0.96, 1.  , 1.  , 1.  ,\n",
      "       0.96, 1.  , 1.  , 1.  , 0.98, 1.  , 1.  , 1.  , 0.98, 1.  , 0.98,\n",
      "       1.  , 0.98, 1.  , 0.98, 1.  , 0.98, 1.  , 0.98, 1.  , 0.98, 1.  ,\n",
      "       0.98, 1.  , 0.98, 1.  , 0.98, 1.  , 0.96, 0.98, 0.98, 0.96, 0.96,\n",
      "       1.  , 0.96, 1.  , 0.96, 1.  , 0.96, 1.  , 0.98, 1.  , 0.96, 1.  ,\n",
      "       0.98, 1.  , 0.98, 1.  , 0.98, 1.  , 0.98, 1.  ]), 'split4_test_score': array([0.32, 0.28, 0.32, 0.3 , 0.32, 0.28, 0.32, 0.3 , 0.32, 0.28, 0.32,\n",
      "       0.3 , 0.36, 0.32, 0.38, 0.36, 0.36, 0.32, 0.38, 0.36, 0.36, 0.32,\n",
      "       0.38, 0.36, 0.44, 0.32, 0.44, 0.32, 0.44, 0.32, 0.44, 0.32, 0.44,\n",
      "       0.32, 0.44, 0.32, 0.42, 0.3 , 0.44, 0.32, 0.42, 0.3 , 0.42, 0.32,\n",
      "       0.42, 0.3 , 0.42, 0.32, 0.3 , 0.28, 0.3 , 0.28, 0.3 , 0.28, 0.3 ,\n",
      "       0.28, 0.3 , 0.28, 0.3 , 0.28, 0.34, 0.28, 0.34, 0.28, 0.34, 0.28,\n",
      "       0.34, 0.28, 0.34, 0.28, 0.34, 0.28, 0.38, 0.3 , 0.34, 0.3 , 0.38,\n",
      "       0.3 , 0.34, 0.3 , 0.38, 0.3 , 0.34, 0.3 , 0.32, 0.3 , 0.32, 0.3 ,\n",
      "       0.32, 0.3 , 0.32, 0.3 , 0.32, 0.3 , 0.32, 0.3 ]), 'mean_test_score': array([0.80054902, 0.79254902, 0.80054902, 0.80054902, 0.79654902,\n",
      "       0.80054902, 0.80454902, 0.80454902, 0.79654902, 0.80054902,\n",
      "       0.80454902, 0.80454902, 0.81254902, 0.80454902, 0.82054902,\n",
      "       0.81654902, 0.81254902, 0.80454902, 0.81654902, 0.81254902,\n",
      "       0.81254902, 0.80054902, 0.81654902, 0.80854902, 0.83254902,\n",
      "       0.80454902, 0.83254902, 0.80854902, 0.83254902, 0.79654902,\n",
      "       0.83254902, 0.79654902, 0.83254902, 0.79654902, 0.83254902,\n",
      "       0.79654902, 0.82854902, 0.80054902, 0.83254902, 0.80454902,\n",
      "       0.82054902, 0.79654902, 0.82854902, 0.80054902, 0.82054902,\n",
      "       0.79654902, 0.82854902, 0.80054902, 0.79654902, 0.79654902,\n",
      "       0.80054902, 0.79654902, 0.79654902, 0.80054902, 0.79654902,\n",
      "       0.80054902, 0.79654902, 0.80054902, 0.79654902, 0.80054902,\n",
      "       0.80454902, 0.80054902, 0.80454902, 0.80054902, 0.80054902,\n",
      "       0.79654902, 0.80054902, 0.79254902, 0.80054902, 0.79654902,\n",
      "       0.80054902, 0.79254902, 0.81254902, 0.79654902, 0.80854902,\n",
      "       0.79654902, 0.81254902, 0.80054902, 0.80454902, 0.80054902,\n",
      "       0.81254902, 0.80054902, 0.80454902, 0.80054902, 0.80054902,\n",
      "       0.80454902, 0.79654902, 0.80454902, 0.80054902, 0.80454902,\n",
      "       0.80054902, 0.80454902, 0.80054902, 0.80454902, 0.80054902,\n",
      "       0.80454902]), 'std_test_score': array([0.24344833, 0.25925253, 0.24344833, 0.25437588, 0.24194107,\n",
      "       0.26455073, 0.24650902, 0.25668404, 0.24194107, 0.26455073,\n",
      "       0.24650902, 0.25668404, 0.23080275, 0.24650902, 0.22531118,\n",
      "       0.23313841, 0.23080275, 0.24650902, 0.22296529, 0.23080275,\n",
      "       0.23080275, 0.24344833, 0.22296529, 0.22767148, 0.20191075,\n",
      "       0.24650902, 0.20191075, 0.24882585, 0.20191075, 0.24194107,\n",
      "       0.20191075, 0.24194107, 0.20191075, 0.24194107, 0.20191075,\n",
      "       0.24194107, 0.20969583, 0.25437588, 0.20191075, 0.24650902,\n",
      "       0.20367898, 0.25293375, 0.20969583, 0.24508588, 0.20367898,\n",
      "       0.25293375, 0.20969583, 0.24508588, 0.25230038, 0.2622508 ,\n",
      "       0.25531762, 0.2622508 , 0.25230038, 0.26455073, 0.25230038,\n",
      "       0.26455073, 0.25230038, 0.26455073, 0.25230038, 0.26455073,\n",
      "       0.23555614, 0.26455073, 0.23555614, 0.26455073, 0.23406642,\n",
      "       0.2622508 , 0.23406642, 0.26079086, 0.23406642, 0.2622508 ,\n",
      "       0.23406642, 0.26079086, 0.21943088, 0.25134733, 0.23797963,\n",
      "       0.25102884, 0.21943088, 0.25437588, 0.23521627, 0.25437588,\n",
      "       0.21943088, 0.25437588, 0.23521627, 0.25437588, 0.24443217,\n",
      "       0.25668404, 0.24161018, 0.25668404, 0.24443217, 0.25668404,\n",
      "       0.24443217, 0.25668404, 0.24443217, 0.25668404, 0.24443217,\n",
      "       0.25668404]), 'rank_test_score': array([45, 94, 45, 45, 74, 45, 27, 27, 74, 45, 27, 27, 17, 27, 11, 14, 17,\n",
      "       27, 14, 17, 17, 45, 14, 24,  1, 27,  1, 24,  1, 74,  1, 74,  1, 74,\n",
      "        1, 74,  8, 45,  1, 27, 11, 74,  8, 45, 11, 74,  8, 45, 74, 74, 45,\n",
      "       74, 74, 45, 74, 45, 74, 45, 74, 45, 27, 45, 27, 45, 45, 74, 45, 94,\n",
      "       45, 74, 45, 94, 17, 74, 24, 74, 17, 45, 27, 45, 17, 45, 27, 45, 45,\n",
      "       27, 74, 27, 45, 27, 45, 27, 45, 27, 45, 27])}\n",
      "\n",
      "Best Model:\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "# Define the hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'learning_rate': [0.1, 0.15, 0.2, 0.25],\n",
    "    'colsample_bytree': [0.5, 0.8],\n",
    "    # 'reg_alpha': [0, 0.5, 1],\n",
    "    # 'reg_lambda': [0, 0.5, 1],\n",
    "    # 'gamma': [0, 0.2],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data (do not run this line if you only want the code for setting up the grid search)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the grid search results\n",
    "print(\"Grid Search Results:\")\n",
    "print(grid_search.cv_results_)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Model:\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65923233-9bfe-41c1-a761-2915d3a5ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6875\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        56\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.69        64\n",
      "   macro avg       0.42      0.39      0.41        64\n",
      "weighted avg       0.74      0.69      0.71        64\n",
      "\n",
      "Test Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00       115\n",
      "   macro avg       1.00      1.00      1.00       115\n",
      "weighted avg       1.00      1.00      1.00       115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance on the validation set\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "validation_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", validation_accuracy)\n",
    "print(\"Validation Report:\")\n",
    "print(validation_report)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"Test Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54288c-1f0d-4495-beb1-51ef1678a125",
   "metadata": {},
   "source": [
    "Validation Accuracy: 0.6936936936936937\n",
    "\n",
    "Validation Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      1.00      0.81        74\n",
    "           1       1.00      0.08      0.15        37\n",
    "\n",
    "    accuracy                           0.69       111\n",
    "   macro avg       0.84      0.54      0.48       111\n",
    "weighted avg       0.79      0.69      0.59       111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b6ea841-d5b0-4439-bfc9-d0b59654ab0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective: binary:logistic\n",
      "base_score: None\n",
      "booster: None\n",
      "callbacks: None\n",
      "colsample_bylevel: None\n",
      "colsample_bynode: None\n",
      "colsample_bytree: 0.5\n",
      "device: None\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: None\n",
      "feature_types: None\n",
      "gamma: None\n",
      "grow_policy: None\n",
      "importance_type: None\n",
      "interaction_constraints: None\n",
      "learning_rate: 0.2\n",
      "max_bin: None\n",
      "max_cat_threshold: None\n",
      "max_cat_to_onehot: None\n",
      "max_delta_step: None\n",
      "max_depth: 3\n",
      "max_leaves: None\n",
      "min_child_weight: None\n",
      "missing: nan\n",
      "monotone_constraints: None\n",
      "multi_strategy: None\n",
      "n_estimators: 100\n",
      "n_jobs: None\n",
      "num_parallel_tree: None\n",
      "random_state: 42\n",
      "reg_alpha: None\n",
      "reg_lambda: None\n",
      "sampling_method: None\n",
      "scale_pos_weight: None\n",
      "subsample: 0.5\n",
      "tree_method: None\n",
      "validate_parameters: None\n",
      "verbosity: None\n"
     ]
    }
   ],
   "source": [
    "# print(best_model)\n",
    "# Get the parameters of the trained model\n",
    "params = best_model.get_params()\n",
    "\n",
    "# Print all the parameters\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b5304-c340-4fad-bcc7-a08e2a775d79",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14af940a-9c31-4c4c-9524-f1c49f9c512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
       "       'Holiday_Flag', 'Sales_Amount_Upper', 'Sales_Amount_Lower', 'State',\n",
       "       'Weekly_Sales_52',\n",
       "       ...\n",
       "       'Unemployment_6', 'Unemployment_5', 'Unemployment_4', 'Unemployment_3',\n",
       "       'Unemployment_2', 'Unemployment_1', 'Week_Of_Month', 'Month', 'Quarter',\n",
       "       'Year'],\n",
       "      dtype='object', length=273)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37959212-eb11-43d6-af5a-2b5431aff3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6875\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        56\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.69        64\n",
      "   macro avg       0.42      0.39      0.41        64\n",
      "weighted avg       0.74      0.69      0.71        64\n",
      "\n",
      "######################################################################################################################################################\n",
      "Test Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       113\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.98       115\n",
      "   macro avg       0.49      0.50      0.50       115\n",
      "weighted avg       0.97      0.98      0.97       115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a narrower set of hyperparameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.2,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.5,\n",
    "}\n",
    "\n",
    "# Train the XGBoost model with the reduced set of hyperparameters\n",
    "model = XGBClassifier(**params, objective=\"binary:logistic\", random_state=42)\n",
    "mod el.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance on the validation set\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "validation_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", validation_accuracy)\n",
    "print(\"Validation Report:\")\n",
    "print(validation_report)\n",
    "\n",
    "# As a final step we will use the best model from the validation step to predict the anomalies on test set.\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "y_pred_test = [1 if prediction==-1 else 0 for prediction in y_pred_test]\n",
    "print(\"###\"*50)\n",
    "# Print the F1-score on Test set\n",
    "print(\"Test Report\\n\", classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae84963-a6bf-434c-b6d0-3f1b7cdd9960",
   "metadata": {},
   "source": [
    "Validation Accuracy: 0.8070175438596491\n",
    "Validation Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.96      0.87        79\n",
    "           1       0.84      0.46      0.59        35\n",
    "\n",
    "    accuracy                           0.81       114\n",
    "   macro avg       0.82      0.71      0.73       114\n",
    "weighted avg       0.81      0.81      0.79       114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12331985-123a-49eb-ae8d-d2c05b40e4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective: binary:logistic\n",
      "base_score: None\n",
      "booster: None\n",
      "callbacks: None\n",
      "colsample_bylevel: None\n",
      "colsample_bynode: None\n",
      "colsample_bytree: 0.5\n",
      "device: None\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: None\n",
      "feature_types: None\n",
      "gamma: None\n",
      "grow_policy: None\n",
      "importance_type: None\n",
      "interaction_constraints: None\n",
      "learning_rate: 0.2\n",
      "max_bin: None\n",
      "max_cat_threshold: None\n",
      "max_cat_to_onehot: None\n",
      "max_delta_step: None\n",
      "max_depth: 3\n",
      "max_leaves: None\n",
      "min_child_weight: None\n",
      "missing: nan\n",
      "monotone_constraints: None\n",
      "multi_strategy: None\n",
      "n_estimators: 100\n",
      "n_jobs: None\n",
      "num_parallel_tree: None\n",
      "random_state: 42\n",
      "reg_alpha: None\n",
      "reg_lambda: None\n",
      "sampling_method: None\n",
      "scale_pos_weight: None\n",
      "subsample: 0.5\n",
      "tree_method: None\n",
      "validate_parameters: None\n",
      "verbosity: None\n"
     ]
    }
   ],
   "source": [
    "# print(best_model)\n",
    "# Get the parameters of the trained model\n",
    "params = model.get_params()\n",
    "\n",
    "# Print all the parameters\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19145be0-d6e8-4101-babd-c180a672c8e0",
   "metadata": {},
   "source": [
    "### Forward Chaining or Rolling Window Split \"Walk Forward Validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d96e7-338a-41c4-a912-3eea3211515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff176c-4be3-41c9-85bd-1f19d26bd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a narrower set of hyperparameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.5,\n",
    "    # 'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "# Define model\n",
    "XGBmodel = XGBClassifier(**params, objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 3\n",
    "\n",
    "# Initialize TimeSeriesSplit object\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Loop over each split and train-validation the model\n",
    "for train_index, validation_index in tscv.split(train_data):\n",
    "    X_train_split, X_validation_split = train_data.iloc[train_index], train_data.iloc[validation_index]\n",
    "    \n",
    "    # Train the model\n",
    "    XGBmodel.fit(X_train_split.drop(['Anomaly','Date'], axis=1), X_train_split['Anomaly'])\n",
    "    \n",
    "    # Predict the anomalies on validation set\n",
    "    y_pred = XGBmodel.predict(X_validation_split.drop(['Anomaly','Date'], axis=1))\n",
    "    \n",
    "    # Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "    y_pred = [1 if prediction==-1 else 0 for prediction in y_pred]\n",
    "    \n",
    "    # Print the F1-score for each validation\n",
    "    print(\"F1-score for each validation\", f1_score(X_validation_split['Anomaly'], y_pred))\n",
    "\n",
    "# As a final step we will use the best model from the validation step to predict the anomalies on test set.\n",
    "y_pred_test = XGBmodel.predict(test_data.drop(['Anomaly','Date'], axis=1))\n",
    "\n",
    "# Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "y_pred_test = [1 if prediction==-1 else 0 for prediction in y_pred_test]\n",
    "\n",
    "# Print the F1-score on Test set\n",
    "print(\"Test F1-score\", classification_report(test_data['Anomaly'], y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7df025-cb67-4c85-a1ff-8501aae46d56",
   "metadata": {},
   "source": [
    "## Train Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3463f2-0013-4473-99a1-127dea4f8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "IFmodel = IsolationForest(contamination=0.01)\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 3\n",
    "\n",
    "# Initialize TimeSeriesSplit object\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Loop over each split and train-validation the model\n",
    "for train_index, validation_index in tscv.split(train_data):\n",
    "    X_train_split, X_validation_split = train_data.iloc[train_index], train_data.iloc[validation_index]\n",
    "    # print(\"train_index, validation_index\",train_index, validation_index)\n",
    "    # print(X_train.shape, X_validation.shape)\n",
    "    # Train the model\n",
    "    IFmodel.fit(X_train_split.drop(['Anomaly','Date'], axis=1))\n",
    "    \n",
    "    # Predict the anomalies on validation set\n",
    "    y_pred = IFmodel.predict(X_validation_split.drop(['Anomaly','Date'], axis=1))\n",
    "    \n",
    "    # Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "    y_pred = [1 if prediction==-1 else 0 for prediction in y_pred]\n",
    "    \n",
    "    # Print the F1-score for each validation\n",
    "    print(\"F1-score for each validation\", f1_score(X_validation_split['Anomaly'], y_pred))\n",
    "\n",
    "# As a final step we will use the best model from the validation step to predict the anomalies on test set.\n",
    "y_pred_test = IFmodel.predict(test_data.drop(['Anomaly','Date'], axis=1))\n",
    "\n",
    "# Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "y_pred_test = [1 if prediction==-1 else 0 for prediction in y_pred_test]\n",
    "\n",
    "# Print the F1-score on Test set\n",
    "print(\"Test F1-score\", classification_report(test_data['Anomaly'], y_pred_test))\n",
    "test_data['Anomaly_pred'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48681b-07a9-4379-980a-44ef1138aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "IFmodel = IsolationForest(contamination=0.01)\n",
    "\n",
    "# X_train, y_train\n",
    "\n",
    "IFmodel.fit(X_train)\n",
    "\n",
    "# Predict the anomalies on validation set\n",
    "y_pred = IFmodel.predict(X_val)\n",
    "\n",
    "# Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "y_pred = [1 if prediction==-1 else 0 for prediction in y_pred]\n",
    "\n",
    "# Print the F1-score for each validation\n",
    "print(\"F1-score validation\", f1_score(y_val, y_pred))\n",
    "\n",
    "# As a final step we will use the best model from the validation step to predict the anomalies on test set.\n",
    "y_pred_test = IFmodel.predict(X_test)\n",
    "\n",
    "# Change the anomaly labels (from -1, 1) to (1, 0) similar to 'Anomaly' column\n",
    "y_pred_test = [1 if prediction==-1 else 0 for prediction in y_pred_test]\n",
    "\n",
    "# Print the F1-score on Test set\n",
    "print(\"Test F1-score\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e67fec-dd36-47fa-8e7e-6ed06bbc4ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be968688-ebe1-41b8-8cd2-3b7e46e308eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80335f-22fa-411f-89d2-c365d84cf65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e037db6-d980-4b16-b0f2-9ddd6d705f50",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732076b-bcb3-4e52-b282-c581e98f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "joblib.dump(model, model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa02fdb-ce5a-4048-8891-d61637405622",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33804608-ad0d-443b-b3c8-092c1c8c146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from file\n",
    "loaded_model = joblib.load(model_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb81627-ee88-4648-a056-02fa1c2bc12b",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b6539-5929-47a0-8242-92b879dda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_accuracy)\n",
    "print(\"Test Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0310f2-1d63-4fff-b31c-d656c61f2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_test_pred2 = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model performance on the test set\n",
    "test_accuracy = accuracy_score(y_test2, y_test_pred2)\n",
    "test_report = classification_report(y_test2, y_test_pred2)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_accuracy)\n",
    "print(\"Test Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5266178-a3f5-45d3-893e-8613520f6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model performance on the test set\n",
    "test_accuracy = accuracy_score(y_test2, y_test_pred)\n",
    "test_report = classification_report(y_test2, y_test_pred)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_accuracy)\n",
    "print(\"Test Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d5ccb-cd98-47b3-89aa-4e85db90c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005f1ec-ab4f-4341-80b9-b02f2edf54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, len(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f4973-770a-44d5-b173-e0a4dcb09544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data['Anomaly_pred'] = y_test_pred\n",
    "test_data[test_data['Anomaly']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa34c1-ff92-4355-8e5e-5f6b8a84a496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_df = X_test\n",
    "# test_df['Anomaly'] = y_test\n",
    "# test_df['Anomaly_pred'] = y_test_pred\n",
    "test_data[(test_data['Anomaly']==1) & (test_data['State']==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652bbb44-fbd4-4e1d-b6cb-456e28b3c3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "for state in test_data['State'].unique():\n",
    "    # Create a Figure for the current state\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Filter the DataFrame for the current state\n",
    "    state_df = test_data[test_data['State'] == state]\n",
    "    state_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Create a Scatter plot for the target values\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=state_df.index,\n",
    "        y=state_df[target],\n",
    "        mode='lines+markers',\n",
    "        name=target,\n",
    "        line=dict(color='black', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Filter the data where anomalies are present (Anomaly == 1)\n",
    "    anomalies = state_df[state_df['Anomaly'] == 1]\n",
    "    \n",
    "    # Add a scatter trace for the anomaly points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=anomalies.index,\n",
    "        y=anomalies[target],\n",
    "        mode='markers',\n",
    "        name='Anomaly',\n",
    "        marker=dict(color='red', size=10, symbol='x')\n",
    "    ))\n",
    "    \n",
    "    # Update layout settings\n",
    "    fig.update_layout(\n",
    "        title=f\"Anomalies for {state}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=target,\n",
    "        template=\"plotly_white\",\n",
    "        autosize=False,\n",
    "        width=1200,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "     # Create a Figure for the current state\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Filter the DataFrame for the current state\n",
    "    state_df = test_data[test_data['State'] == state]\n",
    "    state_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Create a Scatter plot for the target values\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=state_df.index,\n",
    "        y=state_df[target],\n",
    "        mode='lines+markers',\n",
    "        name=target,\n",
    "        line=dict(color='black', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Filter the data where anomalies are present (Anomaly == 1)\n",
    "    anomalies = state_df[state_df['Anomaly_pred'] == 1]\n",
    "    \n",
    "    # Add a scatter trace for the anomaly points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=anomalies.index,\n",
    "        y=anomalies[target],\n",
    "        mode='markers',\n",
    "        name='Anomaly Pred',\n",
    "        marker=dict(color='red', size=10, symbol='x')\n",
    "    ))\n",
    "    \n",
    "    # Update layout settings\n",
    "    fig.update_layout(\n",
    "        title=f\"Predicted Anomalies for {state}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=target,\n",
    "        template=\"plotly_white\",\n",
    "        autosize=False,\n",
    "        width=1200,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "    print(\"####\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc743af3-9cf1-47ec-be34-e9de74e6e97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0586c90-c86a-4671-bb5c-49d7cbbb82fa",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae4ea9-38cb-4fa5-acf6-fa899eb7aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset for inference\n",
    "inference_data = pd.read_csv(inference_file, usecols=['Date','Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
    "       'Holiday_Flag','State'])\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "# inference_data['Date'] = pd.to_datetime(inference_data['Date']).dt.strftime('%Y-%m-%d')\n",
    "inference_data = standardize_date_col(inference_data, 'Date')\n",
    "\n",
    "# Get the saved Encoder for State column \n",
    "encoder = joblib.load(state_encoder)\n",
    "inference_data['State'] = encoder.fit_transform(inference_data['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6d005-4e5b-48fa-a59a-5a9239971740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'].max(), inference_data['Date'].min(), inference_data['Date'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1bf03-2a01-49d0-96b4-770dac7e24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = inference_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3602f1-c1f9-476e-bdaa-d912323f026b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the next four Fridays after 2023-10-27\n",
    "start_date = pd.to_datetime('2023-10-27')\n",
    "next_fridays = [start_date + pd.DateOffset(weeks=i) for i in range(1, 5)]\n",
    "\n",
    "# Function to generate synthetic data with a fluctuation of 5%\n",
    "def generate_synthetic_data(base_data, date):\n",
    "    # Copy the base data to start with\n",
    "    synthetic_data = base_data.copy()\n",
    "    # Update the date to the current date\n",
    "    synthetic_data['Date'] = date.strftime('%Y-%m-%d')\n",
    "    # Apply a fluctuation of ±5% to the numeric columns\n",
    "    fluctuation = np.random.uniform(-0.05, 0.05)\n",
    "    synthetic_data['Weekly_Sales'] *= (1 + np.abs(fluctuation*100))\n",
    "    synthetic_data['Temperature'] *= (1 + fluctuation)\n",
    "    synthetic_data['Fuel_Price'] *= (1 + fluctuation)\n",
    "    synthetic_data['CPI'] *= (1 + fluctuation)\n",
    "    synthetic_data['Unemployment'] *= (1 + fluctuation)\n",
    "    # Return the synthetic data\n",
    "    return synthetic_data\n",
    "\n",
    "# List to hold synthetic data\n",
    "synthetic_data_list = inference_data\n",
    "\n",
    "# Generate synthetic data for each next Friday\n",
    "for date in next_fridays:\n",
    "    synthetic_data = generate_synthetic_data(synthetic_data_list, date)\n",
    "    inference_data = pd.concat([inference_data, synthetic_data])\n",
    "\n",
    "# inference_data = synthetic_data_list\n",
    "# del synthetic_data_list\n",
    "\n",
    "# clean up the data:\n",
    "# Identify numeric columns\n",
    "numeric_columns = inference_data.select_dtypes(include=[int, float]).columns\n",
    "\n",
    "# Round the numeric values in these columns to 2 decimal places\n",
    "inference_data[numeric_columns] = inference_data[numeric_columns].round(2)\n",
    "\n",
    "inference_data.reset_index(inplace=True, drop=True)\n",
    "# Print the synthetic DataFrame\n",
    "inference_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d7329-2a1d-45e8-9f29-cb3e2741941e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_data = standardize_date_col(inference_data, 'Date')\n",
    "inference_data_all = pd.concat([data[cols],inference_data])\n",
    "inference_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26957033-f8a0-4133-92e4-4472ac300d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sort the data by the 'Date' column in ascending order\n",
    "inference_data_all = inference_data_all.sort_values('Date')\n",
    "\n",
    "# Create lag features for the relevant columns\n",
    "print(groupby_cols, lag_columns)\n",
    "inference_data_all = get_lag_columns(inference_data_all, groupby_cols, lag_columns)\n",
    "\n",
    "inference_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f3bd5-558c-49e1-ae35-966a9acb96b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_data_all[inference_data_all['Date']>='2023-10-27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6666b2-9876-4358-b069-28d6fdae301e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the inference data back with lag values:\n",
    "inference_data = inference_data_all[inference_data_all['Date']>='2023-10-27']\n",
    "# Get week of the month value too before passing it to model\n",
    "inference_data = get_date_features(inference_data, 'Date')\n",
    "# inference_data['Week_Of_Month'] = inference_data['Date'].map(week_of_month)\n",
    "# Display the updated new dataset with lag features\n",
    "inference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc08c5-e51f-47aa-8492-ee032335e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b72ac-219b-422f-a4d7-6d99ac4ded61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1112863943\n",
    "\n",
    "inference_data['Weekly_Sales'] = 100\n",
    "inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75582b-4e8e-4b2d-b3e5-f8f0689f5ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform inference using the loaded model\n",
    "predictions = model.predict(inference_data.drop(columns=['Date']))\n",
    "\n",
    "# Display the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb4d8c8-8540-42f1-a9b7-665eae1518b2",
   "metadata": {},
   "source": [
    "## Saving inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15eb43b-1505-4100-942f-9b8f5455f763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e5b7aa-a18a-4f95-8f5e-d605e110d4a6",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
